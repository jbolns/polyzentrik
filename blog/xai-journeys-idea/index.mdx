---
title: 'Brainstorming about AI explainability and interpretability'
date: '2023-10-23'
slug: 'xai-journeys-idea'
type: 'blog'
categories: 'artificial-intelligence'
author: 'J'
author_link: 'https://www.josebolanos.xyz/'
hero_image: './xai-thumb.png'
hero_image_alt: 'An example of how the XAI journey could potentially look at the end of an analysis.'
hero_image_credit_text: 'Polyzentrik'
hero_image_credit_link: 'https://www.polyzentrik.com/'
---

There is a lot to like in the literature about explainable AI (XAI), which closely relates but is not exactly the same than interpretable AI. 

But there seems to be, ironically, a need for better explanation and potentially planning of the process involved in delivering on the promise of explainability and interpretability.

### The symptom
What caught our attention in the XAI world is that even those who are interested in and otherwise meaningfully understand AI can confuse AI explainability with AI interpretability.

Granted, the difference is subtle. But it is not insignificant:
* **AI explainability** is driven by an interest in general but meaningful explanations. For the sake of simplicity and because this is a brainstorming post, one can think about it in terms of summaries – overview of causes and outcomes.
* **AI interpretability** is driven by an interest in detailed insight into what AI does. For simplicity, one can think about it in terms of translation – insight into the logic and process.

Explaining why something happened by way of a summary is a fundamentally different task to translating in detail why something happened. Yet, the terms are regularly conflated. Why? Why would we forget so easily about this subtle yet incredibly meaningful difference?

### The problem (big question mark)
Our best guess at the moment is that there is a fuzzy understanding of the link between the techniques involved in making AI either explainable or interpretable and the process by which these techniques produce outcomes that people consider interpretable/explainable.

In other words. The goals are clear. The techniques exist. But when the latter are applied mechanistically, we risk losing sight of how, when, and what the techniques deliver.

### Breaking the process in small steps
A way to avoid this (or to prove we are wrong and everyone totally gets it) would be to map the steps involved in making an AI explainable/interpretable. 

We belive this can be achieved by using a risk pyramid containing the different levels of (un)explainability/interpretability an AI model can have. It then becomes possible to literally draw the journey by which different techniques make AI models more explainable or interpretable. 

We do this in the image below by imagining how an AI model could be made more explainable by iterative application of different SHAP techniques.

![An image depicting an AI explainability/interpretability risk pyramind and steps by which to make the model more explainable/interpretable.](xai-shap-prelim.png 'Figure 1. XAI journeys - Imaginary SHAP.')

* **STEP 1. Use a standard SHAP to explain decisions by an AI.** This can give you an after-the-fact (ex-post) overview of why outcomes where as such, but the insight may not be complete due to documented shortcomings in SHAP.
* **STEP 2. Use Causal SHAP to enhance insight.** This might help you improve the explanation by offering insight into something SHAP struggles with, i.e., direct/indirect contributions.
* **STEP 3. Use the results of 1 & 2 to calibrate future models.** This might give you a degree of foresight into future outcomes. You might also be able to include some flags that enhance insight into key aspects of the AI process, thereby enabling some minor interpretability.

This can then be communicated in a far prettier manner by including only the relevant steps. 

![An image depicting an stylised AI explainability/interpretability journey.](xai-shap-prelim-clean.png 'Figure 1. XAI journeys - Stylised imaginary SHAP.')

### Launching a new project
Obviously, nothing ever works as first imagined, but we reckon there is something there worth pursuing further.

For this reason, we have made the pyramid above the basis for a new digital ethics project aimed at developing a conceptual module to facilitate planning and communication of explainable/interpretable AI efforts. Read more about this project [here](http://localhost:8000/resources/xai-journeys/).

In theory, the risk pyramid we used in this example can be used to explain any explainability or interpretability journey. We'd like to test that theory through additional brainstorming and potentially some real-world cases.

If the assumption holds, the approach to mapping AI explainability/interpretability journeys shown in figure 1 can potentially help improve communication and planning of XAI (and interpretable AI) efforts.

### Get in touch!
So, get in touch if you think any of the above makes any sense. We think it does.
