---
title: 'XAI Journeys'
date: '2023-10-14'
type: 'resources'
categories: 'digital-ethics'
slug: 'xai-journeys'
author: 'J'
intro: 'A conceptual model to facilitate planning and communication of efforts to make AI more explainable and interpretable.'
status: 'development'
author_link: 'https://www.josebolanos.xyz/'
hero_image: 'images/cat-ethics.png'
hero_image_alt: 'An abstract image of a lightbulb'
hero_image_credit_text: 'Adapted from Free SVG'
hero_image_credit_link: 'https://freesvg.org/'
---

We are developing a conceptual module to facilitate planning and communication of explainable/interpretable artificial intelligence (XAI) efforts. 

This project is currently being developed.

### Justification
There is currently a lot of interest in making AI more explainable (think summary – insight into causes and outcomes) and/or interpretable (think translation – detailed insight into the logic and process).

There are also techniques by which to achieve a degree of AI explainability and/or interpretability, such as decision trees, Grad-CAM (Gradient-weighted Class Activation Mapping ), SHAP (SHapley Additive exPlanations), and LIME (Local Interpretable Model-agnostic Explanations), amongst others.

What is not so clear is the process and extent by which these techniques deliver explainability/interpretability, nor the timelines involved when extremely complex AIs are involved.

We are therefore developing a conceptual model that helps to more easily plan and communicate efforts to make AI more explainable and/or interpretable.

### Status
This project is currently under development. A snapshot of the conceptual model is currently available via our [blog](../../blog/xai-journeys-idea).

More information will become available as and when the project moves forward.

### Usage
**Coding services.** Not yet applicable.

**External usage.** Not yet applicable. 

### Feedback
We welcome feedback via [LinkedIn](https://www.linkedin.com/company/polyzentrik).